{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyM+aBkVr/EzXl3DEBqH4EX8",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Hazuna227/221230039-Pengantar-ML/blob/main/week-02/latihan_praktikum_4_pytorch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "LATIHAN 4: OPERASI PYTORCH UNTUK DEEP LEARNING"
      ],
      "metadata": {
        "id": "8LGCOgt-iotA"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GcMpB7_IiS3c",
        "outputId": "feb822d1-4e9e-41e0-a3d4-e375ba6b14ca"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Linear layer output (first 5 rows):\n",
            " tensor([[ 5.5341],\n",
            "        [-1.2201],\n",
            "        [ 1.9464],\n",
            "        [ 0.8760],\n",
            "        [-5.2849]])\n",
            "\n",
            "✅ ReLU activated output (first 5 rows):\n",
            " tensor([[5.5341],\n",
            "        [0.0000],\n",
            "        [1.9464],\n",
            "        [0.8760],\n",
            "        [0.0000]])\n",
            "\n",
            "✅ Batch normalized X (first 5 rows):\n",
            " tensor([[ 2.2481,  1.1513,  1.1982, -2.1970,  0.8047, -1.4145, -0.1293, -2.2946,\n",
            "         -0.6585,  1.7036],\n",
            "        [-0.0680, -1.7929, -0.5556, -0.6701, -0.6846,  0.4437,  1.2857, -0.2846,\n",
            "         -0.4262,  0.3183],\n",
            "        [-0.4331,  0.7348,  1.0906,  1.5420,  1.4228,  0.9406,  0.4194,  1.7938,\n",
            "         -0.1839, -0.1374],\n",
            "        [ 0.0727,  0.5123, -1.2629, -0.9781, -0.1233,  1.3323,  0.1746, -0.6531,\n",
            "          0.3060, -1.0727],\n",
            "        [-1.2314,  0.6506, -0.7192, -0.7113, -1.2046,  1.7096, -1.1297, -0.7413,\n",
            "         -0.8059, -0.9393]])\n",
            "\n",
            "✅ One-hot encoded labels:\n",
            " tensor([[0., 1., 0.],\n",
            "        [0., 1., 0.],\n",
            "        [0., 0., 1.],\n",
            "        [0., 1., 0.],\n",
            "        [0., 1., 0.],\n",
            "        [1., 0., 0.],\n",
            "        [0., 0., 1.],\n",
            "        [0., 1., 0.],\n",
            "        [1., 0., 0.],\n",
            "        [0., 1., 0.]])\n",
            "\n",
            "✅ Manual matrix multiplication result:\n",
            " tensor([[19., 22.],\n",
            "        [43., 50.]])\n",
            "\n",
            "✅ Torch.matmul result:\n",
            " tensor([[19., 22.],\n",
            "        [43., 50.]])\n",
            "\n",
            "✅ All checks passed\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "\n",
        "# =========================\n",
        "# Simulasi batch data\n",
        "# =========================\n",
        "batch_size, n_features = 32, 10\n",
        "torch.manual_seed(42)  # agar output reproducible\n",
        "X = torch.randn(batch_size, n_features)\n",
        "weights = torch.randn(n_features, 1)\n",
        "bias = torch.randn(1)\n",
        "\n",
        "# =========================\n",
        "# Linear layer\n",
        "# =========================\n",
        "def linear_layer(X, W, b):\n",
        "    return X @ W + b\n",
        "\n",
        "output = linear_layer(X, weights, bias)\n",
        "\n",
        "# =========================\n",
        "# ReLU activation\n",
        "# =========================\n",
        "def relu_activation(tensor):\n",
        "    return torch.maximum(tensor, torch.tensor(0.0))\n",
        "\n",
        "activated = relu_activation(output)\n",
        "\n",
        "# =========================\n",
        "# Batch normalization\n",
        "# =========================\n",
        "def simple_batch_norm(tensor, epsilon=1e-5):\n",
        "    mean = tensor.mean(dim=0, keepdim=True)\n",
        "    std = tensor.std(dim=0, keepdim=True)\n",
        "    return (tensor - mean) / (std + epsilon)\n",
        "\n",
        "normalized = simple_batch_norm(X)\n",
        "\n",
        "# =========================\n",
        "# One-hot encoding\n",
        "# =========================\n",
        "def one_hot_pytorch(labels, num_classes):\n",
        "    one_hot = torch.zeros(labels.size(0), num_classes)\n",
        "    one_hot[torch.arange(labels.size(0)), labels] = 1\n",
        "    return one_hot\n",
        "\n",
        "labels = torch.randint(0, 3, (10,))\n",
        "one_hot = one_hot_pytorch(labels, num_classes=3)\n",
        "\n",
        "# =========================\n",
        "# Print output\n",
        "# =========================\n",
        "print(\"✅ Linear layer output (first 5 rows):\\n\", output[:5])\n",
        "print(\"\\n✅ ReLU activated output (first 5 rows):\\n\", activated[:5])\n",
        "print(\"\\n✅ Batch normalized X (first 5 rows):\\n\", normalized[:5])\n",
        "print(\"\\n✅ One-hot encoded labels:\\n\", one_hot)\n",
        "\n",
        "# =========================\n",
        "# Manual matrix multiplication\n",
        "# =========================\n",
        "def manual_matrix_multiply(A, B):\n",
        "    m, n = A.shape\n",
        "    n2, p = B.shape\n",
        "    if n != n2:\n",
        "        raise ValueError(\"Inner dimensions must match\")\n",
        "    C = torch.zeros(m, p)\n",
        "    for i in range(m):\n",
        "        for j in range(p):\n",
        "            for k in range(n):\n",
        "                C[i, j] += A[i, k] * B[k, j]\n",
        "    return C\n",
        "\n",
        "# Test\n",
        "A = torch.tensor([[1, 2], [3, 4]], dtype=torch.float32)\n",
        "B = torch.tensor([[5, 6], [7, 8]], dtype=torch.float32)\n",
        "manual_result = manual_matrix_multiply(A, B)\n",
        "torch_result = torch.matmul(A, B)\n",
        "\n",
        "print(\"\\n✅ Manual matrix multiplication result:\\n\", manual_result)\n",
        "print(\"\\n✅ Torch.matmul result:\\n\", torch_result)\n",
        "\n",
        "# =========================\n",
        "# Assertions\n",
        "# =========================\n",
        "assert output.shape == (batch_size, 1), \"Linear output shape incorrect\"\n",
        "assert torch.all(activated >= 0), \"ReLU should be >= 0\"\n",
        "assert normalized.shape == X.shape, \"Batch norm should preserve shape\"\n",
        "assert one_hot.shape == (10, 3), \"One-hot shape incorrect\"\n",
        "assert torch.allclose(manual_result, torch_result), \"Manual multiplication incorrect\"\n",
        "print(\"\\n✅ All checks passed\")\n"
      ]
    }
  ]
}